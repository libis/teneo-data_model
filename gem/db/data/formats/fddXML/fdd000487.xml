<?xml version="1.0" encoding="UTF-8"?>
<fdd:FDD id="fdd000487" titleName="Polynomial Texture Map (PTM) File Format" shortName="PTM" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:fdd="http://www.loc.gov/preservation/digital/formats/schemas/fdd/v1" xsi:schemaLocation="http://www.loc.gov/preservation/digital/formats/schemas/fdd/v1 http://www.loc.gov/preservation/digital/formats/schemas/fdd/v1/fdd-v1-1.xsd">
	<fdd:properties>
		<fdd:gdfrGenreSelection>
			<fdd:gdfrGenreAndSubgenres>
				<fdd:gdfrGenre>model</fdd:gdfrGenre>
				<fdd:gdfrSubgenres>
					<fdd:gdfrSubgenre>still-image</fdd:gdfrSubgenre>
				</fdd:gdfrSubgenres>
			</fdd:gdfrGenreAndSubgenres>
		</fdd:gdfrGenreSelection>
		<fdd:formatCategories>
			<fdd:category>file-format</fdd:category>
			<fdd:category>encoding</fdd:category>
		</fdd:formatCategories>
		<fdd:gdfrComposition>unitary</fdd:gdfrComposition>
		<fdd:gdfrConstraint>unstructured</fdd:gdfrConstraint>
		<fdd:gdfrBasis>sampled</fdd:gdfrBasis>
		<fdd:updates>
			<fdd:date>2018-06-14</fdd:date>
		</fdd:updates>
		<fdd:draftStatus>Preliminary</fdd:draftStatus>
	</fdd:properties>
	<fdd:identificationAndDescription>
		<fdd:fullName>Polynomial Texture Map (PTM) File Format</fdd:fullName>
		<fdd:keywords>
			<fdd:keyword/>
		</fdd:keywords>
		<fdd:description>
			<p>The Polynomial Texture Map (PTM) file format is an openly documented image file format published by Tom Malzbender and
Dan Gelb, when they worked for Hewlett-Packard Laboratories (HP Labs).  The <a href="https://web.archive.org/web/20170829021926/https://www.hpl.hp.com/research/ptm/downloads/PtmFormat12.pdf">current version (1.2) of the PTM specification</a> was published in November 2001.  The special functionality of the PTM file format and the related <fddLink id="fdd000486">RTI</fddLink> file format is that, with specialized viewer software, images can be adjusted interactively on screen by a user as if viewing an object with light from varying directions.  This makes the formats very useful for studying objects that are relatively flat, but with an uneven surface.  This description focuses on the use of the format in cultural heritage institutions to generate digital surrogates of artifacts in their collections.  Examples of artifacts for which interactive viewing using PTM images has been particularly valuable include <a href="https://dornsife.usc.edu/wsrp/exquisite-mirror-annenberg-exhibition/">cuneiform tablets</a>, <a href="https://palaeo-electronica.org/2002_1/fossils/issue1_02.htm">fossils</a>, <a href="http://culturalheritageimaging.org/What_We_Do/Fields/numismatics/">coins</a>, and the <a href="https://www.hpl.hp.com/news/2006/oct-dec/antikythera.html">Antikythera mechanism</a>.  The images are sometimes described as 2D+.  They are generated by photographing an object multiple times with lighting direction varying in a controlled fashion between images and fitting a mathematical function to the set of image data points for each pixel.  When Malzbender, Gelb and colleagues developed this imaging technique and the associated PTM file format, HP Labs had an active group working in the area.  See <a href="https://web.archive.org/web/20141103231547/http://www.hpl.hp.com/research/ptm/">Polynomial Texture Mapping (PTM) at HP Labs</a>. Page available via Internet Archive.</p>
			<p>Polynomial Texture Mapping, as an image
capture and processing technique, was first described in <a href="https://www.hpl.hp.com/techreports/2000/HPL-2000-38R1.html">Enhancement of Shape Perception by Surface Reflectance Transformation</a>, a technical report from HP Labs in 2000. For that first explanation, forty images were acquired of a stationary sample, all with identical camera position, but varying in the position of a light source.  A per-pixel reflectance model is derived from these images and transformed to enhance the perception of surface detail and shape.  A PTM file holds functions incorporating effects of lighting direction, not just single color values in <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB</a> or some other color representation, such as <a href="https://en.wikipedia.org/wiki/YCbCr">YCbCr</a>.  In a conventional image, each pixel contains static values for red, green, and blue channels.  In a PTM, each pixel (sometimes called a <i>textel</i> or <i>texel</i>, for &quot;texture element&quot;) holds coefficients that specify a function that calculates the red, green, blue values of that textel as a function of two independent parameters, Lu and Lv, representing the direction from the light source to the textel/pixel.  The curious or mathematically inclined will find links to the underlying mathematics in <a href="#notes">Notes</a> below.</p>
			<p>Typically, PTMs are used for displaying the appearance of an object under varying lighting directions with Lu and Lv specifying the direction of a virtual point light source.  A typical PTM image file stores a set of coefficients per color channel and/or luminance per textel.  These values can be derived mathematically (e.g., by <a href="https://en.wikipedia.org/wiki/Least_mean_squares_filter">least mean squares</a> methods) from the data for the corresponding pixel in each of the source images.</p>
			<p>As described below, the PTM file format supports several different encoding formats for color and luminance.  In the specification for the PTM file format, the term &quot;format&quot; refers to these different encodings; in this description we use &quot;encoding format&quot; for clarity, to distinguish from references to the overall PTM file format.</p>
			<p>A PTM file (always in <a href="https://en.wikipedia.org/wiki/Endianness">little-endian form</a>) begins with the following sections, separated by newline characters (aka line-feed or LF) with an optional space before the newline:</p>
			<ul>
				<li>
					<b>Header String</b>: The ASCII string &apos;PTM_1.2&apos; appears as the first line of the file, identifying the file as a PTM file of the specified version.</li>
				<li>
					<b>Format String</b>:  The ASCII string on the second line
identifying the encoding format used in the file.  See <a href="#notes">Notes</a> below for discussion of the encoding formats permitted.  The encoding format options are for different models for color and luminance changes with lighting direction and for whether or not the data is compressed.  The most commonly used encoding formats are identified by PTM_FORMAT_LRGB (aka LRGB PTM) and PTM_FORMAT_RGB (aka RGB PTM).  The LRGB PTM encoding format focuses on changes in luminance with lighting direction, using static values for the three colors for a textel.  This requires less data than the RGB PTM encoding format, which models the changes in the three colors with lighting direction.  In <a href="https://www.hpl.hp.com/research/ptm/papers/ptm.pdf">Polynomial Texture Maps</a> (presented at Siggraph 2001), the inventors recommend the LRGB PTM variant because &quot;the chromaticity of a particular pixel is fairly constant under varying light source direction; it is largely the luminance that varies.&quot;  They also state, &quot;An RGB PTM is capable of representing color shifts in materials. This characteristic is not common. For the majority of materials an LRGB PTM is adequate and always more efficient.&quot;</li>
				<li>
					<b>Image Size</b>: The next line consists of an ASCII string containing the width and height of the PTM map in pixels. Optionally, a newline may separate the two numbers.</li>
				<li>
					<b>Scale and Bias</b>: The PTM coefficients are stored in the file as a single byte per coefficient, with range of 0 to 1. Scale and bias values are used to allow the stored values to be mapped to the proper values.  This technique is commonly used in graphics applications to optimize computational precision for pixel data given a constrained size for data elements.  A total of 6 bias and 6 scale values are provided, applicable to the 6 polynomial coefficients.  The six ASCII floating point scale values appear first, followed by the six ASCII integer bias values, all separated by
spaces.</li>
			</ul>
			<p>The exact representation for the actual image data depends on the encoding format in use.  The specification indicates, that for the uncompressed encoding formats, the coefficient data is organized by pixel/textel in reversed scanline order (i.e., from bottom to top by row) without line separators.</p>
			<p>In addition to supporting interactive viewing by varying the virtual light source, the PTM file format supports, with computational efficiency, additional techniques that are useful for enhancing the contrast in an image to study details such as inscriptions.  See section 4.1 of <a href="https://www.hpl.hp.com/research/ptm/papers/ptm.pdf">Polynomial Texture Maps</a> for discussion of specular enhancement and diffuse gain.</p>
		</fdd:description>
		<fdd:shortDescription>an openly documented image file format published by Tom Malzbender and Dan Gelb when working at HP Labs. The current version (1.2) of the PTM specification was published in November 2001. The special characteristic of the PTM file format is that with a special viewer, the image can be adjusted on screen by a user to show an object lit from varying directions. This makes the formats very useful for studying objects that are relatively flat, but with an uneven surface.  Examples of objects for which interactive viewing using PTM images has been particularly valuable include cuneiform tablets, fossils, coins, and the Antikythera mechanism. The images are sometimes described as 2D+.</fdd:shortDescription>
		<fdd:productionPhase>A final-state format, intended for use by end users with a special viewer.  Not intended as an archival format.  Cultural Heritage Imaging, a non-profit organization providing support and training for various computational photography techniques, states &quot;Photogrammetry is archive friendly. Strictly speaking, all of the 3D information required to build a scaled, virtual, textured 3D representation is contained in the 2D photos present in a well-designed photogrammetric capture set.&quot;  They recommend retaining the images in the capture set together with information about the capture configuration. This statement and approach applies equally well to Polynomial Texture Mapping or Reflectance Transformation Imaging.  In all cases, as software improves and computational power available increases, even better images or models should be derivable from the same source images.</fdd:productionPhase>
		<fdd:relationships>
			<fdd:relationship>
				<fdd:typeOfRelationship>Affinity to</fdd:typeOfRelationship>
				<fdd:relatedTo>
					<fdd:id>fdd000486</fdd:id>
					<fdd:shortName>RTI</fdd:shortName>
					<fdd:titleName>Reflectance Transformation Imaging (RTI) File Format</fdd:titleName>
				</fdd:relatedTo>
				<fdd:comment>Intended as a more general format for images encoded as coefficients defining a per-pixel reflectance model of the depicted surface.</fdd:comment>
			</fdd:relationship>
		</fdd:relationships>
	</fdd:identificationAndDescription>
	<fdd:localUse>
		<fdd:experience>No direct experience.</fdd:experience>
		<fdd:preference>The Library of Congress has not yet expressed any format preference for digital models used as designs or surrogates for 3-dimensional objects.</fdd:preference>
	</fdd:localUse>
	<fdd:sustainabilityFactors>
		<fdd:disclosure>Openly documented in 2001 at Hewlett-Packard Laboratories, also known as HP Labs.</fdd:disclosure>
		<fdd:documentation>
			<a href="https://web.archive.org/web/20170829021926/https://www.hpl.hp.com/research/ptm/downloads/PtmFormat12.pdf">Polynomial Texture Map (.ptm) File Format, version 1.2, November 2001</a>.  By Tom Malzbender and Dan Gelb of HP Labs. Link available via Internet Archive.</fdd:documentation>
		<fdd:adoption>
			<p>At least two non-profit organizations exist that provide imaging services or consulting to cultural heritage organizations or projects that will benefit from PTM or RTI images.  <a href="http://culturalheritageimaging.org/Technologies/RTI/index.html">Cultural Heritage Imaging</a> (CHI) is based in California.  The <a href="http://vcg.isti.cnr.it/">Visual Computing Lab</a> (VCL-ISTI) at <a href="https://www.isti.cnr.it/en/">ISTI</a> (Istituto di Scienza e Tecnologie
dell’Informazione), an institute of the National Research Council of Italy located in Pisa, has supported projects throughout Europe, including the <a href="https://visual.ariadne-infrastructure.eu/">Ariadne Visual Media Service</a>.  Other projects and organizations that have developed expertise include the Smithsonian Museum Conservation Institute&apos;s <a href="https://www.si.edu/MCIImagingStudio/RTI">Imaging Studio</a>, the <a href="https://dornsife.usc.edu/wsrp/">West Semitic Research Project</a> (WSRP) at the University of Southern California, and the <a href="http://acrg.soton.ac.uk">University of Southampton Archaeological Computing Research Group</a>.</p>
			<p>For a small selection of projects at museums and other archival institutions that have employed PTM imaging, see <a href="#notes">Notes</a> below.</p>
			<p>Instructions and kits for building and using equipment to photograph sets of images suitable for assembling PTM or RTI image files are available from various sources, e.g., <a href="https://dornsife.usc.edu/wsrp/manuals/">WSRP</a>; <a href="https://historicengland.org.uk/images-books/publications/multi-light-imaging-heritage-applications/heag069-multi-light-imaging/">Historic England</a>; <a href="https://hackaday.io/project/11951-affordable-reflectance-transformation-imaging-dome">Affordable Reflectance Transformation Imaging Dome</a> by Leszek Pawlowicz; <a href="https://chsopensource.org/free-rti-training-spheres-set/">Cultural Heritage Science Open Source</a> (CHSOS) and <a href="http://culturalheritageimaging.org/What_We_Offer/Gear/">Cultural Heritage Imaging | Gear</a> (CHI).</p>
			<p>Free software is available for building PTM files from sets of images.  Executables of <a href="https://web.archive.org/web/20190220050723/http://www.hpl.hp.com/research/ptm/HighlightBasedPtms/PTMBuilderSoftwareLicense.htm">PTM Builder</a> and <a href="https://web.archive.org/web/20070114070540/http://www.hpl.hp.com/research/ptm/downloads/download.html">PTM Fitter</a> software can be downloaded from HP Labs.  The download pages for this software are available via Internet Archive.</p>
			<p>Viewers available include:  <a href="https://web.archive.org/web/20070114070540/http://www.hpl.hp.com/research/ptm/downloads/download.html">the original PTM Viewer from HP Labs</a> as executables for Windows, Mac, and Linux (download page available via Internet Archive); <a href="https://github.com/clifflyon/ptmviewer">a port of the original HP Labs code to Java</a>, supporting viewing of PTMs over the web and with source code available; and RTIViewer, an open-source viewer for PTM and RTI images.  See <a href="http://culturalheritageimaging.org/What_We_Offer/Downloads/View/index.html">RTIViewer</a> at CHI to download executables of the viewer or request copies of the source code and <a href="http://vcg.isti.cnr.it/rti/rtiviewer.php">RTI Viewer</a> at VCL-ISTI, for information about sources of technical and financial support for development of the viewer.</p>
			<p>Applications of PTM outside the cultural heritage domain include generation of surfaces responsive to light direction that can be wrapped over models of physical objects used in video games, animated films, or 3-D models used by architects, designers, etc.  PTM &quot;shaders&quot; in the <a href="https://en.wikipedia.org/wiki/OpenGL_Shading_Language">OpenGL Shading Language</a> (used for video games) were developed by Brad Ritter of HP Labs. <a href="https://books.google.com/books?id=lGpP6OpZxLkC&amp;pg=PT471&amp;lpg=PT471">Section 14.4 of OpenGL Shading Language: Polynomial Texture Mapping with BRDF Data</a> states, &quot;One use of PTMs is for representing materials that vary spatially across the surface.  Materials such as brushed metal, woven fabric, wood, and stone, all reflect light differently depending on the viewing angle and light source direction.  They may also have interreflections and self-shadowing.  The PTM captures these details and reproduces them at runtime.&quot;  PTM is not the only models used for such shading, also known as &quot;texture mapping.&quot;  Texture mapping models that respond to variation in viewing direction and light source vary in data size and computational power needed for real-time rendering.  See <a href="http://library.utia.cas.cz/separaty/2009/RO/filip-bidirectional%20texture%20function%20modeling%20state%20of%20the%20art%20survey.pdf">Bidirectional Texture Function Modeling:
A State of the Art Survey</a> from 2009 for a comparison of various models.  The compilers of this resource have not determined how widely used the PTM format is used in video games or design applications.  <a href="../contact_format.shtml">Comments welcome</a>.</p>
		</fdd:adoption>
		<fdd:licensingAndPatents>The format specification includes neither license terms nor explicit copyright statement.  HP makes the associated software executables available under a reasonably generous license for research and education and for &quot;personal and lawful non-commercial use.&quot;  See <a href="https://web.archive.org/web/20190220050723/http://www.hpl.hp.com/research/ptm/HighlightBasedPtms/PTMBuilderSoftwareLicense.htm">http://www.hpl.hp.com/research/ptm/HighlightBasedPtms/PTMBuilderSoftwareLicense.htm</a> and <a href="https://web.archive.org/web/20161219010751/http://www.hpl.hp.com/research/ptm/downloads/agreement.html">http://www.hpl.hp.com/research/ptm/downloads/agreement.html</a>.  (Links available via Internet Archive.)  The license terms for the software include the statement, &quot;No patent license is given by this agreement.&quot;  The inventors at HP Labs applied for three PTM-related patents in March 2000.  The patents were granted and assigned to Hewlett-Packard; two have definitely expired and the other has likely expired, since the patent applications were more than 17 years ago.  See <a href="#useful">Useful References</a> below for more details.  There is no evidence that HP has attempted to exploit the patents when used for scientific or cultural heritage research or conservation.</fdd:licensingAndPatents>
		<fdd:transparency>The PTM format is relatively transparent.  It can be viewed in plain text editors.  The introductory technical metadata are organized in lines and encoded in ASCII.  The coefficient data itself is stored in one-byte numbers in the <i>unsigned char</i> format used in the C programming language.  A very simple program would present these numbers in a human-readable form.  The coefficient data organization is by pixel in reversed scanline order (i.e., from bottom to top).</fdd:transparency>
		<fdd:selfDocumentation>The PTM format has no capabilities for embedding descriptive or contextual metadata.</fdd:selfDocumentation>
		<fdd:externalDependencies>None beyond software that can import and render this format.   Not supported by regular raster image viewers or editors.  Requires a specialized viewer.</fdd:externalDependencies>
		<fdd:techProtection>The PTM format has no support for encryption or other means of technical protection.</fdd:techProtection>
	</fdd:sustainabilityFactors>
	<fdd:qualityAndFunctionalityFactors>
		<fdd:stillImageQF>
			<fdd:normalImage>The PTM image format is not intended as a simple two-dimensional raster image.  However, it is raster-based and can support panning and zooming.  Simple 2D images can be derived for printing, to display in browsers, or to render or manipulate with widely used raster image tools.</fdd:normalImage>
			<fdd:clarity>Good.  Source data appear to be assumed to be 8 bits-per-channel.</fdd:clarity>
			<fdd:colorMaint>No support for ICC color profiles.</fdd:colorMaint>
			<fdd:graphics>No support for vector graphics.</fdd:graphics>
			<fdd:beyondImage>PTM images are intended to capture surface properties and support viewer control with varying virtual light sources.  Characteristics that can be captured include approximately correct diffuse lighting for small-scale features and global effects like self-shadowing.  The images are sometimes described as 2D+.</fdd:beyondImage>
		</fdd:stillImageQF>
	</fdd:qualityAndFunctionalityFactors>
	<fdd:fileTypeSignifiers>
		<fdd:signifiersGroup>
			<fdd:filenameExtension>
				<fdd:sigValues>
					<fdd:sigValue>ptm</fdd:sigValue>
				</fdd:sigValues>
				<fdd:note>Indicated in abstract for the copy of the specification uploaded by the authors to ResearchGate as <a href="https://www.researchgate.net/publication/294629641_Polynomial_texture_map_PTM_file_format">Polynomial texture map (.PTM) file format</a>.  Several other file formats use the same extension.</fdd:note>
			</fdd:filenameExtension>
			<fdd:internetMediaType>
				<fdd:sigValueNA>Not found.</fdd:sigValueNA>
			</fdd:internetMediaType>
			<fdd:magicNumbers>
				<fdd:sigValues>
					<fdd:sigValue>ASCII: PTM_1.2.PTM_FORMAT_</fdd:sigValue>
					<fdd:sigValue>Hex: 50 54 4D 5F 31 2E 32 0A 50 54 4D 5F 46 4F 52 4D 41 54 5F</fdd:sigValue>
				</fdd:sigValues>
				<fdd:note>From PRONOM.  The unprintable linefeed character (Hex 0A) is shown as a period in the ASCII representation.</fdd:note>
			</fdd:magicNumbers>
			<fdd:other>
				<fdd:tag>Pronom PUID</fdd:tag>
				<fdd:values>
					<fdd:sigValues>
						<fdd:sigValue>fmt/519</fdd:sigValue>
					</fdd:sigValues>
					<fdd:note>See <a href="https://www.nationalarchives.gov.uk/pronom/fmt/519">https://www.nationalarchives.gov.uk/pronom/fmt/519</a>.</fdd:note>
				</fdd:values>
			</fdd:other>
			<fdd:other>
				<fdd:tag>Wikidata Title ID</fdd:tag>
				<fdd:values>
					<fdd:sigValues>
						<fdd:sigValue>Q52230534</fdd:sigValue>
					</fdd:sigValues>
					<fdd:note>For Polynomial Texture Map.  See <a href="https://www.wikidata.org/wiki/Q52230534">https://www.wikidata.org/wiki/Q52230534</a>.</fdd:note>
				</fdd:values>
			</fdd:other>
		</fdd:signifiersGroup>
	</fdd:fileTypeSignifiers>
	<fdd:notes>
		<fdd:general>
			<p>
				<b>PTM image encoding formats</b>: The PTM &apos;formats&apos; described in the PTM specification and supported by HP&apos;s PTM Viewer are listed in a table in <a href="http://culturalheritageimaging.org/What_We_Do/Publications/eurographics2008/eurographics_2008_tutorial_notes.pdf">Image-Based Empirical Information Acquisition, Scientific 
Reliability, and Long-Term Digital Preservation for the 
Natural Sciences and Cultural Heritage</a>, from which the information below is derived.  The encoding formats most commonly used are the first two listed below:</p>
			<ul>
				<li>PTM_FORMAT_LRGB.  Luminance as a polynomial multiplied by unscaled RGB. 9 (6+3) bytes per pixel.</li>
				<li>PTM_FORMAT_RGB.  Polynomial coefficients for each color channel. 18 (3x6) bytes per pixel.</li>
				<li>PTM_FORMAT_LUM. YCrCb
 color space, only Y as a polynomial.  1 or 2 bytes per pixel.</li>
				<li>PTM_FORMAT_PTM_LUT. Index to a lookup table that contains RGB values plus polynomial coefficients.  4 (3+1) bytes per pixel.</li>
				<li>PTM_FORMAT_PTM_C_LUT.  RGB values plus an index to a lookup table that contains only 
polynomial coefficients.  Variable bytes per pixel.</li>
				<li>PTM_FORMAT_JPEG_RGB.  JPEG compression of PTM_FORMAT_RGB. Variable bytes per pixel.</li>
				<li>PTM_FORMAT_JPEG_LRGB.  JPEG compression of PTM_FORMAT_LRGB. Variable bytes per pixel.</li>
				<li>PTM_FORMAT_JPEGLS_RGB.  JPEGLS compression of  PTM_FORMAT_RGB. Variable bytes per pixel.</li>
				<li>PTM_FORMAT_JPEGLS_LRGB.  JPEGLS compression of PTM_FORMAT_LRGB. Variable bytes per pixel.</li>
			</ul>
			<p>
				<b>Mathematics underlying PTM</b>:  The mathematical details underlying the PTM methodology (for the LRGB PTM encoding format) are described in section 3.2 (Polynomial Color Dependence) in <a href="https://web.archive.org/web/20141103231547/http://www.hpl.hp.com/research/ptm/">Polynomial Texture Maps</a>, a paper by the inventors at Siggraph 2001, available via Internet Archive.  For each pixel with position (u, v) in the image plane, luminosity (L) is modelled with a biquadratic polynomial as a function of incoming light direction. The direction is given by the projection (Lu, Lv) of the light vector (normalized to unit length) onto the image plane (see <a href="https://palaeo-electronica.org/content/images/384/fig_3.jpg">diagram</a> from the article <a href="https://palaeo-electronica.org/content/component/content/category/173-384">Virtual whitening of fossils using polynomial texture mapping</a>).  The six coefficients for the polynomial (usually referred to as a0 through a5) are fitted to the photographic data per texel/pixel and stored in the PTM.  Malzbender et al computed the best fit
using singular value decomposition (<a href="https://en.wikipedia.org/wiki/Singular-value_decomposition">SVD</a>) to solve for a0-a5.  The SVD is computed only once given an arrangement of light sources for the set of capture photographs and then can be applied per pixel.  The PTM format was specifically designed to allow surface colors to be efficiently reconstructed from these coefficients and light directions with minimal fixed-point hardware.</p>
			<p>A useful summary is found in a sequence of slides from a presentation by Pradeep Rajiv entitled <a href="http://slideplayer.com/slide/10418516/">Image based PTM synthesis for realistic rendering of low resolution 3D models</a>.  <a href="https://images.slideplayer.com/35/10418516/slides/slide_32.jpg">Slide 32</a> illustrates the use of multiple images (in this case, only three images) from the same viewpoint, but lit from different directions; <a href="https://images.slideplayer.com/35/10418516/slides/slide_33.jpg">Slide 33</a> summarizes the information stored in the PTM (in this case, an LRGB PTM);  <a href="https://images.slideplayer.com/35/10418516/slides/slide_34.jpg">Slide 34</a> shows the equations used to calculate the luminance (L) and the color channel intensities (R, G, and B) from the PTM for each texel/pixel given a virtual light direction. Note that, in this last slide, the term &quot;texture&quot; is used instead of &quot;image,&quot; because the author is using the PTM technique to generate a 2-dimensional &quot;texture&quot; to wrap over a 3-dimensional model (as in virtual reality applications) rather than a 2-dimensional view on a screen.</p>
			<p>
				<b>Use of the PTM file format by archival institutions</b>: Described briefly below are some examples of use of the PTM technique and format to create digital surrogates of artifacts in collections of museums and other archival institutions.  The surrogates are used both for scholarly study and for presentations for the general public.</p>
			<p>The Freer Gallery of Art and the Arthur M. Sackler Gallery Archives (Freer|Sackler Archives) hold a significant collection of 393 squeezes from ancient archaeological sites in the Near East. A squeeze is a series of sheets of paper that are layered on top of each other and moistened to create a wet pulp.  This substance is pressed onto the inscriptions, creating a paper mold and capturing the writing for a 3-dimensional effect.  In 2010, the Freer|Sackler Archives received a grant from the Smithsonian Institution&apos;s Collections Care and Preservation Fund to aid in the preservation of the squeezes and the 3-D information they contain. The Freer|Sackler Archives collaborated with the Smithsonian Institution Museum Conservation Institute (MCI) to create a digital preservation surrogate for each squeeze, using Reflectance Transformation Imaging (RTI).  See <a href="https://web.archive.org/web/20191013110829/http://archive.asia.si.edu/research/squeezeproject/">Squeeze Imaging Project</a> for information about the project, available via Internet Archive.</p>
			<p>In 2005, the HP Labs team worked with the Antikythera Mechanism Research Project to generate PTMs of the fragments of <a href="https://en.wikipedia.org/wiki/Antikythera_mechanism">Antikythera Mechanism</a>.  For information about this project, see the <a href="https://web.archive.org/web/20190124141109/http://www.hpl.hp.com/research/ptm/antikythera_mechanism/full_resolution_ptm.htm">Antikythera</a> page at HP Labs and <a href="http://www.antikythera-mechanism.gr/data/ptm/full-resolution-ptm">Full resolution PTM</a> at the Antikythera Mechanism Research Project in Greece.  These pages are available via Internet Archive.</p>
			<p>PTM and RTI have been used effectively by archaeologists to document stone tools.  See <a href="https://rtimage.us/?page_id=18">Reflectance Transformation Imaging For Lithics</a> from Dr. Leszek Pawlowicz.</p>
			<p>Cultural Heritage Imaging (CHI) has worked with curators and art conservators to demonstrate the potential use of RTI to help document and conserve works of art in their collections.  See <a href="https://vimeo.com/12753104">Documentary: RTI and Art Conservation (2010)</a> and <a href="http://culturalheritageimaging.org/What_We_Do/Fields/conservation/">Art Conservation (at CHI)</a>.</p>
			<p>CHI experimented in the imaging of coins using PTM in collaboration with Tom Malzbender and Dan Gelb at Hewlett-Packard Labs and then explored imaging possibilities further in a project documented in <a href="http://culturalheritageimaging.org/What_We_Do/Publications/vast2005/VAST2005_final.pdf">Reflection Transformation Imaging and Virtual Representations of Coins from the Hospice of the Grand St. Bernard</a>. See also <a href="http://culturalheritageimaging.org/What_We_Do/Fields/numismatics/">Numismatics: The Study of Coins</a> at Cultural Heritage Imaging.</p>
			<p>A 2017 article, <a href="https://www.spiedigitallibrary.org/journals/Journal-of-Electronic-Imaging/volume-26/issue-1/011029/Underwater-reflectance-transformation-imaging--a-technology-for-italicin-situ/10.1117/1.JEI.26.1.011029.short">Underwater reflectance transformation imaging: a technology for in situ underwater cultural heritage object-level recording</a> describes successful experiments with the use of RTI techniques under water, using the PTM format for the images.</p>
		</fdd:general>
		<fdd:history>Improvements to the PTM fitting algorithm have been suggested.  One example is presented by Mark Drew et al. in <a href="https://www.semanticscholar.org/paper/Robust-estimation-of-surface-properties-and-of-Drew-Hel-Or/3213b826eb864cbace0be1c5062abb4340a837e0?p2df">Robust estimation of surface properties and interpolation
of shadow/specularity components</a>, claiming increased accuracy of surface
normals, albedo, and color in generated images.  Additional articles on this &quot;robust&quot; methodology by Mark Drew and colleague are listed below in <a href="#useful">Useful References</a>. The compilers of this description have not been able to determine whether the PTM file format can be used to record the output of these algorithms or whether publicly available tools exist to support use of the techniques by others.  <a href="../contact_format.shtml">Comments welcome</a>.</fdd:history>
	</fdd:notes>
	<fdd:formatSpecifications>
		<fdd:urls>
			<fdd:url>
				<fdd:urlReference>
					<link>https://web.archive.org/web/20170829021926/https://www.hpl.hp.com/research/ptm/downloads/PtmFormat12.pdf</link>
					<tag>Polynomial Texture Map (.ptm) File Format (2001)</tag>
					<comment>Link available via Internet Archive.</comment>
				</fdd:urlReference>
			</fdd:url>
		</fdd:urls>
	</fdd:formatSpecifications>
	<fdd:usefulReferences>
		<fdd:urls>
			<fdd:url>
				<fdd:urlReference>
					<link>https://www.hpl.hp.com/techreports/2000/HPL-2000-38R1.html</link>
					<tag>Enhancement of Shape Perception by Surface Reflectance Transformation.  By Tom Malzbender et al.  (2000)</tag>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://www.hpl.hp.com/research/ptm/papers/ptm.pdf</link>
					<tag>Polynomial Texture Maps (2001).  By Tom Malzbender, Dan Gelb, Hans Wolters (HP Labs)</tag>
					<comment>Paper presented at Siggraph 2001</comment>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://eprints.soton.ac.uk/156253/</link>
					<tag>Archaeological applications of polynomial texture mapping: analysis, conservation and representation. 2010</tag>
					<comment>Article by Graeme Earl, Kirk Martinez, and Tom Malzbender.   Journal of Archaeological Science, 37 (8), 2040-2050.</comment>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://web.archive.org/web/20141103231547/http://www.hpl.hp.com/research/ptm/</link>
					<tag>Polynomial Texture Mapping (PTM) at HP Labs</tag>
					<comment>Page available via Internet Archive. Links to other pages may no longer be available.</comment>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlGroup>
					<fdd:intro>Hewlett-Packard press releases related to applications of PTM by HP Labs staff.</fdd:intro>
					<fdd:urlList>
						<fdd:urlReference>
							<link>https://www.hpl.hp.com/news/2000/oct-dec/3dimaging.html</link>
							<tag>Picturing the Past | HP Labs Technology Helping Scholars Decipher Ancient Texts (December 2000</tag>
						</fdd:urlReference>
						<fdd:urlReference>
							<link>https://www.hpl.hp.com/news/2006/oct-dec/antikythera.html</link>
							<tag>Imaging technology helps crack ancient mystery
 | Antikythera Mechanism is believed to be the world&apos;s oldest computer (December 2006)</tag>
						</fdd:urlReference>
					</fdd:urlList>
				</fdd:urlGroup>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://graphics.stanford.edu/courses/cs448-05-winter/papers/nicodemus-brdf-nist.pdf</link>
					<tag>Geometrical Considerations and Nomenclature for Reflectance, 
U.S. Dept. of  Commerce, National Bureau of Standards,  October 1977.</tag>
					<comment>Describes some of the underlying mathematics for RTI, including the Bidirectional Reflectance Distribution Function</comment>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlGroup>
					<fdd:intro>Cultural Heritage Imaging (CHI) is a nonprofit organization, dedicated to advancing the state of the art of digital capture and documentation of the world’s cultural, historic, and artistic treasures.  Below are some particular CHI links that relate to the PTM format.  Note that the PTM technique is an example of Reflectance Transformation Imaging.</fdd:intro>
					<fdd:urlList>
						<fdd:urlReference>
							<link>http://culturalheritageimaging.org/Technologies/RTI/index.html</link>
							<tag>Reflectance Transformation Imaging (RTI)</tag>
						</fdd:urlReference>
						<fdd:urlReference>
							<link>http://culturalheritageimaging.org/What_We_Do/Publications/eurographics2008/eurographics_2008_tutorial_notes.pdf</link>
							<tag>Tutorial Notes: Image-Based Empirical Information Acquisition, Scientific 
Reliability, and Long-Term Digital Preservation for the Natural Sciences and Cultural Heritage.</tag>
						</fdd:urlReference>
						<fdd:urlReference>
							<link>http://culturalheritageimaging.org/What_We_Offer/Downloads/</link>
							<tag>CHI Tool Downloads | Capture, RTIBuilder, RTIViewer</tag>
							<comment>These tools can be used to generate PTM files, when combined with tools from HP Labs.</comment>
						</fdd:urlReference>
					</fdd:urlList>
				</fdd:urlGroup>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://en.wikipedia.org/wiki/Polynomial_texture_mapping</link>
					<tag>Wikipedia entry for Polynomial Texture Mapping</tag>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlGroup>
					<fdd:intro>The West Semitic Research Project at the University of Southern California has used RTI techniques and RTI and PTM formats and has offered training programs in capture and processing for PTM image files.</fdd:intro>
					<fdd:urlList>
						<fdd:urlReference>
							<link>http://wsrp.usc.edu/projects/index.shtml</link>
							<tag>A Training Program for Scholars, Conservators and Researchers in the Use of Reflectance Transformation Imaging (RTI) for Documenting Ancient Texts and Artifacts Including the Loan of Imaging Equipment</tag>
							<comment>Has links to documentation and demonstration videos. This page was no longer available when checked in July 2021.</comment>
						</fdd:urlReference>
						<fdd:urlReference>
							<link>https://dornsife.usc.edu/wsrp/manuals/</link>
							<tag>Manuals and links associated with RTI training program</tag>
						</fdd:urlReference>
						<fdd:urlReference>
							<link>https://dornsife.usc.edu/wsrp/videos/</link>
							<tag>Instructional Videos on RTI</tag>
						</fdd:urlReference>
					</fdd:urlList>
				</fdd:urlGroup>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://historicengland.org.uk/images-books/publications/multi-light-imaging-heritage-applications/heag069-multi-light-imaging/</link>
					<tag>Multi-light Imaging for Heritage Applications.  Historic England, 2013</tag>
					<comment>Historic England is the new name for the organization formerly known as English Heritage.</comment>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://web.archive.org/web/20191013110829/http://archive.asia.si.edu/research/squeezeproject/</link>
					<tag>Smithsonian Institution | Squeeze Imaging Project</tag>
					<comment>The RTI technique and format were used to create digital surrogates for the inscription squeezes. Link available via Internet Archive.</comment>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://www.youtube.com/watch?v=rxNg-tXPPWc</link>
					<tag>Imaging the Antikythera Mechanism | Google Tech Talk by Tom Malzbender, March 5, 2010</tag>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://rtimage.us/</link>
					<tag>Reflectance Transformation Imaging for Lithics.  Dr. Leszek M. Pawlowicz</tag>
					<comment>Includes samples of RTI and PTM files.</comment>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://palaeo-electronica.org/2002_1/fossils/issue1_02.htm</link>
					<tag>Imaging Fossils Using Reflectance Transformation and Interactive Manipulation of Virtual Light Sources (2002)</tag>
					<comment>By Øyvind Hammer, Stefan Bengtson, Tom Malzbender, Dan Gelb. Palaeontologia Electronica,   23 August  2002</comment>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://www.economist.com/science-and-technology/2010/03/25/shining-a-light-on-the-past</link>
					<tag>Shining a light on the past: Archaeology and polynomial texture mapping (2010)</tag>
					<comment>Economist,  Mar 25th 2010</comment>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://www.spiedigitallibrary.org/journals/Journal-of-Electronic-Imaging/volume-26/issue-1/011029/Underwater-reflectance-transformation-imaging--a-technology-for-italicin-situ/10.1117/1.JEI.26.1.011029.short</link>
					<tag>Underwater reflectance transformation imaging: a technology for in situ underwater cultural heritage object-level recording (2017)</tag>
					<comment>David Selmo et al. Journal of Electronic Imaging 26(1), Jan/Feb 2017.</comment>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlGroup>
					<fdd:intro>Mark S. Drew of Simon Fraser University and colleagues have worked on some modifications to the PTM methodology, described in the articles below.  The first of these has the objective of improving PTM  imaging for artifacts with specularity (highlights on shiny surfaces) and self-shadowing (for artifacts with more complex shapes).  The third explores improvements in computational efficiency.</fdd:intro>
					<fdd:urlList>
						<fdd:urlReference>
							<link>https://www.semanticscholar.org/paper/Robust-estimation-of-surface-properties-and-of-Drew-Hel-Or/3213b826eb864cbace0be1c5062abb4340a837e0?p2df</link>
							<tag>Robust estimation of surface properties and interpolation
of shadow/specularity components (2012)
</tag>
							<comment>M. S. Drew et al. Image Vision Computing
30(4):317–331 (2012).
  </comment>
						</fdd:urlReference>
						<fdd:urlReference>
							<link>https://www2.cs.sfu.ca/~mark/ftp/Cpcv2012/cpcv2012a.pdf</link>
							<tag>Robust Luminance and Chromaticity for Matte Regression in Polynomial Texture Mapping (2012)</tag>
							<comment>Mingjing Zhang and Mark S. Drew.</comment>
						</fdd:urlReference>
						<fdd:urlReference>
							<link>https://www.researchgate.net/publication/270666069_Efficient_robust_image_interpolation_and_surface_properties_using_polynomial_texture_mapping</link>
							<tag>Efficient robust image interpolation and surface properties using polynomial texture mapping (2014)</tag>
							<comment>Mingjing Zhang and Mark Drew.  EURASIP Journal on Image and Video Processing. 2014(1):25.  Includes comparison with RTI image-fitting using hemispherical harmonics. </comment>
						</fdd:urlReference>
					</fdd:urlList>
				</fdd:urlGroup>
			</fdd:url>
			<fdd:url>
				<fdd:urlGroup>
					<fdd:intro>A 2009 article comparing different texture mapping models, including PTM.</fdd:intro>
					<fdd:urlList>
						<fdd:urlReference>
							<link>http://library.utia.cas.cz/separaty/2009/RO/filip-bidirectional%20texture%20function%20modeling%20state%20of%20the%20art%20survey.pdf</link>
							<tag>Bidirectional Texture Function Modeling:
A State of the Art Survey
(2009)</tag>
							<comment>By Jiří Filip and Michal Haindl.  IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 11, Nov 2009, pp. 1921-1940.</comment>
						</fdd:urlReference>
						<fdd:urlReference>
							<link>https://www.researchgate.net/figure/BTF-results-of-all-eight-compared-methods-mapped-on-a-car-gearbox-console-for-six_fig10_26820502</link>
							<tag>Figure showing texture wrapping for various texture models, including PTM.</tag>
						</fdd:urlReference>
					</fdd:urlList>
				</fdd:urlGroup>
			</fdd:url>
			<fdd:url>
				<fdd:urlGroup>
					<fdd:intro>Tom Malzbender, Dan Gelb, and Hans Wolters filed three associated patent applications in March 2000, assigning them to Hewlett-Packard.  According to the Google Patent site, two of them have definitely expired.  The first was transferred to Siemens and then to Qualcomm, and may since have expired.  <a href="../contact_format.shtml">Comments welcome</a>.</fdd:intro>
					<fdd:urlList>
						<fdd:urlReference>
							<link>https://patents.google.com/patent/US6515674B1/en</link>
							<tag>U. S. Patent: US6515674B1 (2000-03-17): Apparatus for and of rendering 3d objects with parametric texture maps</tag>
							<comment>Assigned to Qualcomm Inc. in 2013.  May have expired in 2017.</comment>
						</fdd:urlReference>
						<fdd:urlReference>
							<link>https://patents.google.com/patent/US6583790B1/en</link>
							<tag>U. S. Patent: US6583790B1 (2000-03-17):  Apparatus for and method of converting height fields into parametric texture maps</tag>
							<comment>Expired in 2011 due to failure to pay maintenance fee.</comment>
						</fdd:urlReference>
						<fdd:urlReference>
							<link>https://patents.google.com/patent/US6654013B1/en</link>
							<tag>U. S. Patent: US6654013B1 (2000-03-17):  Apparatus for and method of enhancing shape perception with parametric texture maps</tag>
							<comment>Expired in 2011 due to failure to pay maintenance fee</comment>
						</fdd:urlReference>
					</fdd:urlList>
				</fdd:urlGroup>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://www.nationalarchives.gov.uk/pronom/fmt/519</link>
					<tag>PRONOM entry for Polynomial Texture Map</tag>
					<comment>PUID for Polynomial Texture Map (PTM) is fmt/519</comment>
				</fdd:urlReference>
			</fdd:url>
			<fdd:url>
				<fdd:urlReference>
					<link>https://www.wikidata.org/wiki/Q52230534</link>
					<tag>Wikidata entry for Polynomial Texture Map</tag>
					<comment>Wikidata Title ID: Q52230534</comment>
				</fdd:urlReference>
			</fdd:url>
		</fdd:urls>
	</fdd:usefulReferences>
</fdd:FDD>
